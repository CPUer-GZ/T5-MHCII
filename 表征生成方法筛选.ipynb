{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import h5py\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_h5_to_dataframe(h5_file_path):\n",
    "    \"\"\"\n",
    "    从 HDF5 文件中读取 peptide_encodings, mhcii_encodings 和 labels，并转换为 Pandas DataFrame。\n",
    "    :param h5_file_path: HDF5 文件的路径\n",
    "    :return: 包含 peptide_encodings, mhcii_encodings 和 label 的 DataFrame\n",
    "    \"\"\"\n",
    "    # 打开 HDF5 文件进行读取\n",
    "    with h5py.File(h5_file_path, 'r') as h5f:\n",
    "        # 打印文件中的数据集名称\n",
    "        print(f\"Datasets in HDF5 file: {list(h5f.keys())}\")\n",
    "\n",
    "        # 读取肽段和MHCII的编码数据集，以及标签数据集\n",
    "        peptide_encodings = h5f['peptide_encodings'][:]\n",
    "        mhcii_encodings = h5f['mhcii_encodings'][:]\n",
    "        labels = h5f['labels'][:]\n",
    "\n",
    "        print(f\"Peptide Encodings Shape: {peptide_encodings.shape}\")\n",
    "        print(f\"MHCII Encodings Shape: {mhcii_encodings.shape}\")\n",
    "        print(f\"Labels Shape: {labels.shape}\")\n",
    "\n",
    "        # 如果 peptide_encodings 或 mhcii_encodings 是三维的，则沿着序列长度维度取平均，转换为二维\n",
    "        if peptide_encodings.ndim == 3:\n",
    "            peptide_encodings = np.mean(peptide_encodings, axis=1)\n",
    "\n",
    "        if mhcii_encodings.ndim == 3:\n",
    "            mhcii_encodings = np.mean(mhcii_encodings, axis=1)\n",
    "\n",
    "    # 将 peptide_encodings, mhcii_encodings 和 labels 转换为 DataFrame\n",
    "    data_df = pd.DataFrame({\n",
    "        'peptide_encoding': list(peptide_encodings),  # 将肽段编码转换为列表形式\n",
    "        'mhcii_encoding': list(mhcii_encodings),  # 将MHCII编码转换为列表形式\n",
    "        'label': labels  # 直接使用标签数组\n",
    "    })\n",
    "\n",
    "    return data_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDF5 文件的大致内容结构：\n",
    "/ (Root)\n",
    "    embeddings (Dataset)\n",
    "        Shape: (N, D)  # N 是样本数，D 是每个序列的编码维度\n",
    "        Type: float32  # 数据类型为 32 位浮点数\n",
    "    labels (Dataset)\n",
    "        Shape: (N,)  # N 是样本数，与 embeddings 数据集中的样本数对应\n",
    "        Type: float32  # 标签数据类型为 32 位浮点数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BA_file = \"D:\\\\OneDrive\\\\我要毕业\\\\new_representation_data\\\\APAAC.h5\" # 替换为你的 HDF5 文件路径\n",
    "data_df = load_data_from_h5_to_dataframe(BA_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_train_error(X_train, y_train, model):\n",
    "    '''returns in-sample error for already fit model.'''\n",
    "    predictions = model.predict(X_train)\n",
    "    mse = mean_squared_error(y_train, predictions)\n",
    "    mae = mean_absolute_error(y_train, predictions)\n",
    "    corr = scipy.stats.pearsonr(y_train, predictions)\n",
    "    return mse,mae,corr\n",
    "\n",
    "def calc_validation_error(X_test, y_test, model):\n",
    "    '''returns out-of-sample error for already fit model.'''\n",
    "    predictions = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    corr = scipy.stats.pearsonr(y_test, predictions)\n",
    "    return mse,mae,corr\n",
    "\n",
    "def calc_metrics(X_train, y_train, X_test, y_test, model):\n",
    "    '''fits model and returns the metrics for in-sample error and out-of-sample error'''\n",
    "    model.fit(X_train, y_train)\n",
    "    train_mse_error,train_mae_error,train_corr = calc_train_error(X_train, y_train, model)\n",
    "    val_mse_error,val_mae_error,val_corr = calc_validation_error(X_test, y_test, model)\n",
    "    return train_mse_error, val_mse_error, train_mae_error, val_mae_error,train_corr,val_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_results(\n",
    "    train_mse_error_list,\n",
    "    validation_mse_error_list,\n",
    "    train_mae_error_list,\n",
    "    validation_mae_error_list,\n",
    "    train_corr_list,\n",
    "    validation_corr_list,\n",
    "    train_corr_pval_list,\n",
    "    validation_corr_pval_list,\n",
    "):\n",
    "    result_df = pd.DataFrame(\n",
    "        {\n",
    "            \"train_mse_error\": round(np.mean(train_mse_error_list) * 100, 4),\n",
    "            \"train_mse_std\": round(np.std(train_mse_error_list) * 100, 4),\n",
    "            \"val_mse_error\": round(np.mean(validation_mse_error_list) * 100, 4),\n",
    "            \"val_mse_std\": round(np.std(validation_mse_error_list) * 100, 4),\n",
    "            \"train_mae_error\": round(np.mean(train_mae_error_list) * 100, 4),\n",
    "            \"train_mae_std\": round(np.std(train_mae_error_list) * 100, 4),\n",
    "            \"val_mae_error\": round(np.mean(validation_mae_error_list) * 100, 4),\n",
    "            \"val_mae_std\": round(np.std(validation_mae_error_list) * 100, 4),\n",
    "            \"train_corr\": round(np.mean(train_corr_list), 4),\n",
    "            \"train_corr_pval\": round(np.mean(train_corr_pval_list), 4),\n",
    "            \"validation_corr\": round(np.mean(validation_corr_list), 4),\n",
    "            \"validation_corr_pval\": round(np.mean(validation_corr_pval_list), 4),\n",
    "        },\n",
    "        index=[0],\n",
    "    )\n",
    "\n",
    "    result_detail_df = pd.DataFrame(\n",
    "        {\n",
    "            \"train_mse_errors\": list(np.multiply(train_mse_error_list, 100)),\n",
    "            \"val_mse_errors\": list(np.multiply(validation_mse_error_list, 100)),\n",
    "            \"train_mae_errors\": list(np.multiply(train_mae_error_list, 100)),\n",
    "            \"val_mae_errors\": list(np.multiply(validation_mae_error_list, 100)),\n",
    "            \"train_corrs\": list(np.multiply(train_corr_list, 100)),\n",
    "            \"train_corr_pvals\": list(np.multiply(train_corr_pval_list, 100)),\n",
    "            \"validation_corr\": list(np.multiply(validation_corr_list, 100)),\n",
    "            \"validation_corr_pval\": list(np.multiply(validation_corr_pval_list, 100)),\n",
    "        },\n",
    "        index=range(len(train_mse_error_list)),\n",
    "    )\n",
    "    return result_df, result_detail_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictAffinityWithModel(regressor_model):\n",
    "    K = 10\n",
    "    kf = KFold(n_splits=K, shuffle=True, random_state=42)\n",
    "\n",
    "    train_mse_error_list = []\n",
    "    validation_mse_error_list = []\n",
    "    train_mae_error_list = []\n",
    "    validation_mae_error_list = []\n",
    "    train_corr_list = []\n",
    "    validation_corr_list = []\n",
    "    train_corr_pval_list = []\n",
    "    validation_corr_pval_list = []\n",
    "\n",
    "    data = np.array(data_df['embedding'].tolist(), dtype=float)\n",
    "    target = np.array(data_df['label'].tolist(), dtype=float)\n",
    "\n",
    "    # 使用 tqdm 包裹 KFold 进度条\n",
    "    with tqdm.tqdm(total=K, desc=\"K-Fold Progress\", unit=\"fold\") as pbar:\n",
    "        for train_index, val_index in kf.split(data, target):\n",
    "\n",
    "            # split data\n",
    "            X_train, X_val = data[train_index], data[val_index]\n",
    "            y_train, y_val = target[train_index], target[val_index]\n",
    "\n",
    "            # instantiate model\n",
    "            reg = regressor_model # linear_model.BayesianRidge()\n",
    "\n",
    "            # calculate error_list\n",
    "            train_mse_error,val_mse_error,train_mae_error,val_mae_error,train_corr,val_corr = calc_metrics(X_train, y_train, X_val, y_val, reg)\n",
    "\n",
    "            # append to appropriate list\n",
    "            train_mse_error_list.append(train_mse_error)\n",
    "            validation_mse_error_list.append(val_mse_error)\n",
    "\n",
    "            train_mae_error_list.append(train_mae_error)\n",
    "            validation_mae_error_list.append(val_mae_error)\n",
    "\n",
    "            train_corr_list.append(train_corr[0])\n",
    "            validation_corr_list.append(val_corr[0])\n",
    "\n",
    "            train_corr_pval_list.append(train_corr[1])\n",
    "            validation_corr_pval_list.append(val_corr[1])\n",
    "\n",
    "    return report_results(\n",
    "        train_mse_error_list,\n",
    "        validation_mse_error_list,\n",
    "        train_mae_error_list,\n",
    "        validation_mae_error_list,\n",
    "        train_corr_list,\n",
    "        validation_corr_list,\n",
    "        train_corr_pval_list,\n",
    "        validation_corr_pval_list,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress:   0%|          | 0/10 [00:00<?, ?fold/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.03144493\n",
      "Iteration 2, loss = 0.02810487\n",
      "Iteration 3, loss = 0.02683461\n",
      "Iteration 4, loss = 0.02599480\n",
      "Iteration 5, loss = 0.02529420\n",
      "Iteration 6, loss = 0.02472152\n",
      "Iteration 7, loss = 0.02433612\n",
      "Iteration 8, loss = 0.02396042\n",
      "Iteration 9, loss = 0.02368381\n",
      "Iteration 10, loss = 0.02340641\n",
      "Iteration 11, loss = 0.02319870\n",
      "Iteration 12, loss = 0.02301860\n",
      "Iteration 13, loss = 0.02272572\n",
      "Iteration 14, loss = 0.02255641\n",
      "Iteration 15, loss = 0.02235680\n",
      "Iteration 16, loss = 0.02222214\n",
      "Iteration 17, loss = 0.02205446\n",
      "Iteration 18, loss = 0.02195190\n",
      "Iteration 19, loss = 0.02187787\n",
      "Iteration 20, loss = 0.02168048\n",
      "Iteration 21, loss = 0.02160088\n",
      "Iteration 22, loss = 0.02153291\n",
      "Iteration 23, loss = 0.02143141\n",
      "Iteration 24, loss = 0.02138966\n",
      "Iteration 25, loss = 0.02127558\n",
      "Iteration 26, loss = 0.02127073\n",
      "Iteration 27, loss = 0.02106488\n",
      "Iteration 28, loss = 0.02106195\n",
      "Iteration 29, loss = 0.02098541\n",
      "Iteration 30, loss = 0.02096866\n",
      "Iteration 31, loss = 0.02087014\n",
      "Iteration 32, loss = 0.02084188\n",
      "Iteration 33, loss = 0.02078447\n",
      "Iteration 34, loss = 0.02072964\n",
      "Iteration 35, loss = 0.02072121\n",
      "Iteration 36, loss = 0.02069878\n",
      "Iteration 37, loss = 0.02062036\n",
      "Iteration 38, loss = 0.02059411\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.03009506\n",
      "Iteration 2, loss = 0.02711298\n",
      "Iteration 3, loss = 0.02579766\n",
      "Iteration 4, loss = 0.02507201\n",
      "Iteration 5, loss = 0.02449259\n",
      "Iteration 6, loss = 0.02405204\n",
      "Iteration 7, loss = 0.02374357\n",
      "Iteration 8, loss = 0.02344950\n",
      "Iteration 9, loss = 0.02315581\n",
      "Iteration 10, loss = 0.02289907\n",
      "Iteration 11, loss = 0.02265922\n",
      "Iteration 12, loss = 0.02247062\n",
      "Iteration 13, loss = 0.02235592\n",
      "Iteration 14, loss = 0.02219714\n",
      "Iteration 15, loss = 0.02207723\n",
      "Iteration 16, loss = 0.02192436\n",
      "Iteration 17, loss = 0.02186127\n",
      "Iteration 18, loss = 0.02167317\n",
      "Iteration 19, loss = 0.02164780\n",
      "Iteration 20, loss = 0.02151078\n",
      "Iteration 21, loss = 0.02144941\n",
      "Iteration 22, loss = 0.02135102\n",
      "Iteration 23, loss = 0.02124089\n",
      "Iteration 24, loss = 0.02113918\n",
      "Iteration 25, loss = 0.02113465\n",
      "Iteration 26, loss = 0.02107312\n",
      "Iteration 27, loss = 0.02094388\n",
      "Iteration 28, loss = 0.02092308\n",
      "Iteration 29, loss = 0.02082601\n",
      "Iteration 30, loss = 0.02081211\n",
      "Iteration 31, loss = 0.02074348\n",
      "Iteration 32, loss = 0.02065966\n",
      "Iteration 33, loss = 0.02060243\n",
      "Iteration 34, loss = 0.02058207\n",
      "Iteration 35, loss = 0.02048452\n",
      "Iteration 36, loss = 0.02046659\n",
      "Iteration 37, loss = 0.02041092\n",
      "Iteration 38, loss = 0.02039081\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.03036956\n",
      "Iteration 2, loss = 0.02732988\n",
      "Iteration 3, loss = 0.02625094\n",
      "Iteration 4, loss = 0.02543608\n",
      "Iteration 5, loss = 0.02480678\n",
      "Iteration 6, loss = 0.02428171\n",
      "Iteration 7, loss = 0.02391655\n",
      "Iteration 8, loss = 0.02363454\n",
      "Iteration 9, loss = 0.02330084\n",
      "Iteration 10, loss = 0.02306353\n",
      "Iteration 11, loss = 0.02277514\n",
      "Iteration 12, loss = 0.02254082\n",
      "Iteration 13, loss = 0.02234339\n",
      "Iteration 14, loss = 0.02236197\n",
      "Iteration 15, loss = 0.02206805\n",
      "Iteration 16, loss = 0.02196990\n",
      "Iteration 17, loss = 0.02188027\n",
      "Iteration 18, loss = 0.02170463\n",
      "Iteration 19, loss = 0.02160089\n",
      "Iteration 20, loss = 0.02150914\n",
      "Iteration 21, loss = 0.02144622\n",
      "Iteration 22, loss = 0.02127532\n",
      "Iteration 23, loss = 0.02123963\n",
      "Iteration 24, loss = 0.02120508\n",
      "Iteration 25, loss = 0.02103704\n",
      "Iteration 26, loss = 0.02103595\n",
      "Iteration 27, loss = 0.02092282\n",
      "Iteration 28, loss = 0.02083319\n",
      "Iteration 29, loss = 0.02081975\n",
      "Iteration 30, loss = 0.02078349\n",
      "Iteration 31, loss = 0.02070929\n",
      "Iteration 32, loss = 0.02066219\n",
      "Iteration 33, loss = 0.02057727\n",
      "Iteration 34, loss = 0.02052587\n",
      "Iteration 35, loss = 0.02051814\n"
     ]
    }
   ],
   "source": [
    "model = MLPRegressor(\n",
    "    hidden_layer_sizes=(256,64),  # 两层隐藏层，分别有 256 和 64 个神经元\n",
    "    activation='relu',             # 使用 ReLU 激活函数\n",
    "    solver='adam',                 # 使用 Adam 优化器\n",
    "    max_iter=100,                  # 最大迭代次数 500\n",
    "    verbose=True                  # 输出训练过程的日志信息\n",
    ")\n",
    "\n",
    "result_df, result_detail_df = predictAffinityWithModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result dataframes to CSV files\n",
    "result_df_file = f\"D:\\\\OneDrive\\\\results\\\\APAAC_result.csv\" \n",
    "result_detail_df_file = f\"D:\\\\OneDrive\\\\results\\\\APAAC_result_detail.csv\"\n",
    "result_df.to_csv(result_df_file, index=False)\n",
    "result_detail_df.to_csv(result_detail_df_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
